# Alternative Methodologies for Systematic Academic Image Collection: Hethport Database Framework

This comprehensive research provides systematic, ethically-compliant methodologies for academic image collection from restricted databases, with complete R implementation supporting semi-automated workflows that combine human oversight with programmatic efficiency.

## Research context and legal framework

**Robots.txt compliance represents industry best practice rather than legal requirement**, but academic researchers must navigate complex intellectual property landscapes. My analysis of current legal precedents reveals that robots.txt files, while not legally binding, establish social norms that responsible researchers should respect. The hiQ v. LinkedIn cases (2019-2022) clarify that scraping publicly available data generally doesn't violate the Computer Fraud and Abuse Act, but contract claims through terms of service violations remain viable.

**Fair use doctrine strongly supports academic research**, particularly transformative uses involving analysis, criticism, and commentary. For cuneiform databases specifically, systematic collection for comparative analysis typically qualifies as fair use under the four-factor test, especially when focused on academic rather than commercial purposes. However, researchers must still respect explicit licensing terms and consider international regulations like GDPR for EU-based databases.

**Institutional Review Board (IRB) requirements depend on data characteristics**. Collections involving publicly available archaeological materials typically don't require IRB review, but researchers should consult early in projects involving any potentially sensitive data or innovative collection methodologies.

## Database access and institutional collaboration opportunities

**Hethport database analysis reveals significant collaboration potential**. The Hittite Local Cults Database operates under Creative Commons Attribution-ShareAlike 4.0 International license, suggesting openness to academic partnerships. Primary contacts include Prof. Michele Cammarosano at University of Würzburg (michele.cammarosano@unior.it) and developer Christoph Forster at datalino.de.

**ADW Mainz institutional framework supports research partnerships** through their Academy Programme and Digital Humanities initiatives. Their €520,000 Hittite digitization project demonstrates scale and commitment to digital scholarship. Contact through their General Secretary (generalsekretariat@adwmainz.de) offers pathways for formal collaboration discussions.

**Successful collaboration models from CDLI and DARIAH-EU** demonstrate that academic databases increasingly support programmatic access through formal partnerships, API development collaboration, and shared infrastructure investments. The key lies in articulating mutual benefits including joint publications, technical contributions, and long-term sustainability commitments.

## Technical implementation strategies

**Browser automation tool selection favors modern R packages**. My research identifies selenider as the optimal choice for new projects, providing user-friendly wrappers for both chromote and selenium with lazy element handling and automatic waiting capabilities. Chromote offers faster performance through Chrome DevTools Protocol but with browser limitations, while RSelenium provides full WebDriver functionality for complex workflows.

**Semi-automated methodologies prove most effective** by combining human judgment with programmatic efficiency. This approach addresses robots.txt concerns by maintaining human oversight while leveraging automation for repetitive tasks like URL management, metadata capture, and systematic download processes.

**Session management and authentication require sophisticated approaches** including cookie persistence through file-based storage or user data directories, session validation functions, and re-authentication wrappers to handle expired sessions gracefully.

## Complete R implementation

Below is the comprehensive R code that implements all requested functionality:

```r
# ==============================================================================
# HETHPORT ACADEMIC IMAGE COLLECTION FRAMEWORK
# Systematic Semi-Automated Collection with Ethical Compliance
# ==============================================================================

# Required Libraries
install.packages(c(
  "R6", "httr", "jsonlite", "readr", "dplyr", "stringr", "digest", 
  "progressr", "lubridate", "magrittr", "robotstxt", "polite", 
  "selenider", "chromote"
))
library(selenider)
library(chromote)
library(polite)
library(robotstxt)
library(httr)
library(jsonlite)
library(readr)
library(dplyr)
library(stringr)
library(digest)
library(progressr)
library(R6)
library(lubridate)
library(magrittr)

# ==============================================================================
# 1. ETHICAL COMPLIANCE AND ROBOTS.TXT CHECKER
# ==============================================================================

EthicalComplianceChecker <- R6Class("EthicalComplianceChecker",
  public = list(
    base_url = NULL,
    robots_content = NULL,
    crawl_delay = 2,
    
    initialize = function(base_url) {
      self$base_url <- base_url
      self$check_robots_txt()
    },
    
    check_robots_txt = function() {
      message("Checking robots.txt compliance...")
      
      # Check robots.txt using robotstxt package
      robots_allowed <- paths_allowed(
        paths = "/", 
        domain = self$base_url, 
        bot = "*"
      )
      
      if (!robots_allowed) {
        stop("robots.txt prohibits access. Consider contacting database administrators.")
      }
      
      # Get robots.txt content for delay information
      self$robots_content <- get_robotstxt(self$base_url)
      
      # Extract crawl delay if specified
      if (grepl("crawl-delay", self$robots_content, ignore.case = TRUE)) {
        delay_match <- regmatches(self$robots_content, 
                                 regexpr("crawl-delay:\\s*(\\d+)", self$robots_content, ignore.case = TRUE))
        if (length(delay_match) > 0) {
          self$crawl_delay <- as.numeric(gsub("crawl-delay:\\s*", "", delay_match, ignore.case = TRUE))
        }
      }
      
      message(paste("robots.txt compliance: APPROVED. Crawl delay:", self$crawl_delay, "seconds"))
      return(TRUE)
    },
    
    get_delay = function() {
      return(self$crawl_delay)
    }
  )
)

# ==============================================================================
# 2. SESSION MANAGEMENT WITH PERSISTENCE
# ==============================================================================

SessionManager <- R6Class("SessionManager",
  public = list(
    session = NULL,
    profile_dir = NULL,
    cookies_file = NULL,
    authenticated = FALSE,
    
    initialize = function(headless = TRUE, profile_name = "hethport_session") {
      # Create persistent profile directory
      self$profile_dir <- file.path(tempdir(), profile_name)
      dir.create(self$profile_dir, showWarnings = FALSE, recursive = TRUE)
      self$cookies_file <- file.path(self$profile_dir, "cookies.json")
      
      # Initialize browser session
      self$session <- selenider_session(
        "chromote",
        options = chromote_options(
          headless = headless,
          browser_args = c(
            paste0("--user-data-dir=", self$profile_dir),
            "--profile-directory=Default",
            "--disable-web-security",
            "--disable-dev-shm-usage"
          )
        )
      )
      
      message("Browser session initialized with persistent profile")
    },
    
    save_cookies = function() {
      if (is.null(self$session)) return(FALSE)
      
      tryCatch({
        cookies <- self$session$driver$Network$getCookies()
        write_json(cookies$result$cookies, self$cookies_file, auto_unbox = TRUE)
        message("Session cookies saved successfully")
        return(TRUE)
      }, error = function(e) {
        warning(paste("Failed to save cookies:", e$message))
        return(FALSE)
      })
    },
    
    load_cookies = function() {
      if (!file.exists(self$cookies_file)) return(FALSE)
      
      tryCatch({
        cookies <- read_json(self$cookies_file, simplifyVector = TRUE)
        for (cookie in cookies) {
          self$session$driver$Network$setCookie(
            name = cookie$name,
            value = cookie$value,
            domain = cookie$domain,
            path = cookie$path %||% "/",
            secure = cookie$secure %||% FALSE,
            httpOnly = cookie$httpOnly %||% FALSE
          )
        }
        message("Session cookies loaded successfully")
        return(TRUE)
      }, error = function(e) {
        warning(paste("Failed to load cookies:", e$message))
        return(FALSE)
      })
    },
    
    authenticate = function(login_url, username = NULL, password = NULL) {
      if (is.null(username) || is.null(password)) {
        # Interactive authentication
        open_url(login_url)
        message("Please complete authentication in the browser window.")
        readline("Press Enter after authentication is complete...")
      } else {
        # Automated authentication
        open_url(login_url)
        Sys.sleep(2)
        
        # Fill credentials
        s("#username") %>% elem_set_value(username)
        s("#password") %>% elem_set_value(password)
        s("#login-button") %>% elem_click()
        
        # Wait for redirect
        Sys.sleep(3)
      }
      
      self$authenticated <- self$validate_authentication()
      if (self$authenticated) {
        self$save_cookies()
        message("Authentication successful and cookies saved")
      }
      
      return(self$authenticated)
    },
    
    validate_authentication = function() {
      tryCatch({
        # Check for authentication indicators
        result <- self$session$driver$Runtime$evaluate(
          "document.querySelector('.user-info, .logout, .authenticated') !== null"
        )
        return(result$result$value)
      }, error = function(e) {
        return(FALSE)
      })
    },
    
    ensure_authenticated = function() {
      if (!self$validate_authentication()) {
        message("Session expired, re-authentication required")
        return(FALSE)
      }
      return(TRUE)
    }
  )
)

# ==============================================================================
# 3. URL QUEUE MANAGEMENT SYSTEM
# ==============================================================================

URLQueue <- R6Class("URLQueue",
  public = list(
    queue_dir = NULL,
    queue_file = NULL,
    processed_file = NULL,
    failed_file = NULL,
    
    initialize = function(queue_dir = "url_queue") {
      self$queue_dir <- queue_dir
      dir.create(queue_dir, showWarnings = FALSE, recursive = TRUE)
      
      self$queue_file <- file.path(queue_dir, "pending_urls.csv")
      self$processed_file <- file.path(queue_dir, "processed_urls.csv")
      self$failed_file <- file.path(queue_dir, "failed_urls.csv")
      
      # Initialize queue files
      if (!file.exists(self$queue_file)) {
        write_csv(data.frame(
          url = character(0), 
          added_time = character(0), 
          priority = integer(0),
          metadata = character(0)
        ), self$queue_file)
      }
    },
    
    add_urls = function(urls, priority = 1, metadata = NULL) {
      new_entries <- data.frame(
        url = urls,
        added_time = as.character(Sys.time()),
        priority = priority,
        metadata = metadata %||% "",
        url_hash = sapply(urls, digest::digest),
        stringsAsFactors = FALSE
      )
      
      existing_queue <- read_csv(self$queue_file, show_col_types = FALSE)
      updated_queue <- rbind(existing_queue, new_entries)
      
      # Remove duplicates and sort by priority
      updated_queue <- updated_queue[!duplicated(updated_queue$url_hash), ]
      updated_queue <- updated_queue[order(-updated_queue$priority, updated_queue$added_time), ]
      
      write_csv(updated_queue, self$queue_file)
      message(sprintf("Added %d URLs to queue (total: %d)", length(urls), nrow(updated_queue)))
      
      return(nrow(updated_queue))
    },
    
    get_next_url = function() {
      queue_data <- read_csv(self$queue_file, show_col_types = FALSE)
      if (nrow(queue_data) == 0) return(NULL)
      
      next_entry <- queue_data[1, ]
      remaining_queue <- queue_data[-1, ]
      write_csv(remaining_queue, self$queue_file)
      
      return(list(
        url = next_entry$url,
        metadata = next_entry$metadata,
        added_time = next_entry$added_time
      ))
    },
    
    mark_processed = function(url, success = TRUE, error_msg = NULL, file_path = NULL) {
      processed_entry <- data.frame(
        url = url,
        processed_time = as.character(Sys.time()),
        success = success,
        error_message = error_msg %||% "",
        file_path = file_path %||% "",
        stringsAsFactors = FALSE
      )
      
      target_file <- if (success) self$processed_file else self$failed_file
      
      if (file.exists(target_file)) {
        existing_data <- read_csv(target_file, show_col_types = FALSE)
        updated_data <- rbind(existing_data, processed_entry)
      } else {
        updated_data <- processed_entry
      }
      
      write_csv(updated_data, target_file)
    },
    
    get_queue_status = function() {
      pending <- nrow(read_csv(self$queue_file, show_col_types = FALSE))
      
      processed <- if (file.exists(self$processed_file)) {
        nrow(read_csv(self$processed_file, show_col_types = FALSE))
      } else 0
      
      failed <- if (file.exists(self$failed_file)) {
        nrow(read_csv(self$failed_file, show_col_types = FALSE))
      } else 0
      
      return(list(pending = pending, processed = processed, failed = failed))
    },
    
    export_urls = function(output_file = "collected_urls.csv") {
      all_files <- list(self$queue_file, self$processed_file, self$failed_file)
      all_urls <- data.frame()
      
      for (file in all_files) {
        if (file.exists(file)) {
          data <- read_csv(file, show_col_types = FALSE)
          if (nrow(data) > 0) {
            data$status <- basename(tools::file_path_sans_ext(file))
            all_urls <- rbind(all_urls, data)
          }
        }
      }
      
      if (nrow(all_urls) > 0) {
        write_csv(all_urls, output_file)
        message(sprintf("Exported %d URLs to %s", nrow(all_urls), output_file))
      }
      
      return(all_urls)
    }
  )
)

# ==============================================================================
# 4. INTERACTIVE URL COLLECTION SYSTEM
# ==============================================================================

InteractiveCollector <- R6Class("InteractiveCollector",
  public = list(
    session_manager = NULL,
    url_queue = NULL,
    
    initialize = function(session_manager, url_queue) {
      self$session_manager <- session_manager
      self$url_queue <- url_queue
    },
    
    collect_urls_interactively = function(base_url, selector = "img[src*='jpg'], img[src*='png'], img[src*='tif'], img[src*='jpeg']") {
      message("Starting interactive URL collection...")
      message("Navigation Instructions:")
      message("  'c' - Collect image URLs from current page")
      message("  'n' - Navigate manually to next page (no action taken)")
      message("  'v' - View current page URLs without collecting")
      message("  's' - Show collection statistics")
      message("  'q' - Quit collection session")
      
      open_url(base_url)
      collected_count <- 0
      
      repeat {
        cat("\n" %+% "="*50 %+% "\n")
        user_input <- tolower(trimws(readline("Action [c/n/v/s/q]: ")))
        
        if (user_input == "c") {
          # Extract and collect URLs
          urls <- self$extract_current_page_urls(selector)
          if (length(urls) > 0) {
            # Get page metadata
            page_title <- self$get_page_title()
            page_url <- self$get_current_url()
            
            # Add to queue with metadata
            metadata <- sprintf("Source: %s | Page: %s", page_url, page_title)
            self$url_queue$add_urls(urls, priority = 1, metadata = metadata)
            
            collected_count <- collected_count + length(urls)
            message(sprintf("✓ Collected %d URLs from current page (Total: %d)", 
                          length(urls), collected_count))
          } else {
            message("✗ No image URLs found on current page")
          }
          
        } else if (user_input == "v") {
          # View URLs without collecting
          urls <- self$extract_current_page_urls(selector)
          if (length(urls) > 0) {
            message(sprintf("Found %d image URLs on current page:", length(urls)))
            for (i in seq_along(urls)) {
              cat(sprintf("  %d. %s\n", i, urls[i]))
              if (i >= 5 && length(urls) > 5) {
                cat(sprintf("  ... and %d more URLs\n", length(urls) - 5))
                break
              }
            }
          } else {
            message("No image URLs found on current page")
          }
          
        } else if (user_input == "s") {
          # Show statistics
          status <- self$url_queue$get_queue_status()
          message(sprintf("Collection Statistics:"))
          message(sprintf("  Pending: %d", status$pending))
          message(sprintf("  Processed: %d", status$processed))
          message(sprintf("  Failed: %d", status$failed))
          message(sprintf("  Total collected this session: %d", collected_count))
          
        } else if (user_input == "n") {
          message("→ Navigate manually to next page, then press 'c' to collect URLs")
          
        } else if (user_input == "q") {
          break
          
        } else {
          message("Invalid input. Use: c (collect), n (navigate), v (view), s (stats), q (quit)")
        }
      }
      
      final_status <- self$url_queue$get_queue_status()
      message(sprintf("\nCollection session complete. Final queue status: %d pending", 
                     final_status$pending))
      
      return(final_status)
    },
    
    extract_current_page_urls = function(selector) {
      tryCatch({
        urls <- self$session_manager$session$driver$Runtime$evaluate(sprintf(
          "Array.from(document.querySelectorAll('%s')).map(img => img.src)", 
          selector
        ))$result$value
        
        # Filter and validate URLs
        valid_urls <- urls[sapply(urls, function(url) {
          !is.na(url) && url != "" && grepl("^https?://", url)
        })]
        
        return(unique(valid_urls))
      }, error = function(e) {
        warning(paste("Error extracting URLs:", e$message))
        return(character(0))
      })
    },
    
    get_page_title = function() {
      tryCatch({
        title <- self$session_manager$session$driver$Runtime$evaluate(
          "document.title || 'Unknown'"
        )$result$value
        return(title)
      }, error = function(e) {
        return("Unknown")
      })
    },
    
    get_current_url = function() {
      tryCatch({
        url <- self$session_manager$session$driver$Runtime$evaluate(
          "window.location.href"
        )$result$value
        return(url)
      }, error = function(e) {
        return("Unknown")
      })
    }
  )
)

# ==============================================================================
# 5. FILE ORGANIZATION WITH FUND NUMBER NAMING
# ==============================================================================

FileOrganizer <- R6Class("FileOrganizer",
  public = list(
    base_dir = NULL,
    fund_number = NULL,
    project_code = NULL,
    
    initialize = function(base_dir = "hethport_collection", fund_number = "FUND001", project_code = "HETH") {
      self$base_dir <- base_dir
      self$fund_number <- fund_number
      self$project_code <- project_code
      
      # Create directory structure
      dir.create(file.path(base_dir, "images"), showWarnings = FALSE, recursive = TRUE)
      dir.create(file.path(base_dir, "metadata"), showWarnings = FALSE, recursive = TRUE)
      dir.create(file.path(base_dir, "documentation"), showWarnings = FALSE, recursive = TRUE)
      dir.create(file.path(base_dir, "logs"), showWarnings = FALSE, recursive = TRUE)
    },
    
    generate_filename = function(url, index = NULL, metadata = NULL) {
      # Extract file extension
      ext <- tools::file_ext(url)
      if (ext == "") ext <- "jpg"
      
      # Generate systematic filename: PROJECT_FUND_DATE_SEQUENCE_TYPE_VERSION
      date_str <- format(Sys.Date(), "%Y%m%d")
      sequence <- if (!is.null(index)) sprintf("%06d", index) else format(Sys.time(), "%H%M%S")
      
      # Clean metadata for filename if provided
      metadata_str <- ""
      if (!is.null(metadata) && metadata != "") {
        clean_meta <- str_replace_all(metadata, "[^A-Za-z0-9_-]", "_")
        clean_meta <- str_trunc(clean_meta, 20)
        metadata_str <- paste0("_", clean_meta)
      }
      
      filename <- sprintf("%s_%s_%s_%s%s_v01.%s", 
                         self$project_code, 
                         self$fund_number,
                         date_str,
                         sequence,
                         metadata_str,
                         ext)
      
      return(filename)
    },
    
    get_file_path = function(filename, subdir = "images") {
      return(file.path(self$base_dir, subdir, filename))
    },
    
    save_metadata = function(url, filename, metadata = NULL, source_info = NULL) {
      meta_data <- list(
        filename = filename,
        source_url = url,
        collection_date = as.character(Sys.time()),
        fund_number = self$fund_number,
        project_code = self$project_code,
        metadata = metadata,
        source_info = source_info
      )
      
      meta_filename <- paste0(tools::file_path_sans_ext(filename), "_meta.json")
      meta_path <- self$get_file_path(meta_filename, "metadata")
      
      write_json(meta_data, meta_path, auto_unbox = TRUE, pretty = TRUE)
      return(meta_path)
    },
    
    create_manifest = function() {
      image_files <- list.files(file.path(self$base_dir, "images"), full.names = TRUE)
      
      manifest_data <- data.frame(
        filename = basename(image_files),
        full_path = image_files,
        size_bytes = file.size(image_files),
        created_time = file.mtime(image_files),
        fund_number = self$fund_number,
        project_code = self$project_code,
        stringsAsFactors = FALSE
      )
      
      manifest_file <- file.path(self$base_dir, "documentation", "collection_manifest.csv")
      write_csv(manifest_data, manifest_file)
      
      message(sprintf("Manifest created with %d files: %s", nrow(manifest_data), manifest_file))
      return(manifest_file)
    }
  )
)

# ==============================================================================
# 6. IMAGE DOWNLOAD ENGINE WITH ERROR HANDLING
# ==============================================================================

ImageDownloader <- R6Class("ImageDownloader",
  public = list(
    session_manager = NULL,
    file_organizer = NULL,
    max_retries = 3,
    delay_between_downloads = 2,
    
    initialize = function(session_manager, file_organizer, max_retries = 3, delay = 2) {
      self$session_manager <- session_manager
      self$file_organizer <- file_organizer
      self$max_retries <- max_retries
      self$delay_between_downloads <- delay
    },
    
    download_image = function(url, index = NULL, metadata = NULL) {
      filename <- self$file_organizer$generate_filename(url, index, metadata)
      dest_path <- self$file_organizer$get_file_path(filename)
      
      # Skip if file already exists
      if (file.exists(dest_path)) {
        return(list(
          success = TRUE, 
          path = dest_path, 
          message = "Already exists",
          filename = filename
        ))
      }
      
      retry_count <- 0
      
      while (retry_count <= self$max_retries) {
        result <- self$attempt_download(url, dest_path)
        
        if (result$success) {
          # Save metadata
          meta_path <- self$file_organizer$save_metadata(url, filename, metadata)
          
          return(list(
            success = TRUE,
            path = dest_path,
            filename = filename,
            size = file.size(dest_path),
            metadata_path = meta_path,
            message = "Downloaded successfully"
          ))
        }
        
        retry_count <- retry_count + 1
        if (retry_count <= self$max_retries) {
          Sys.sleep(2^retry_count)  # Exponential backoff
        }
      }
      
      return(list(
        success = FALSE, 
        message = "Max retries exceeded",
        filename = filename
      ))
    },
    
    attempt_download = function(url, dest_path) {
      tryCatch({
        # Get cookies from session if authenticated
        headers <- list("User-Agent" = "Mozilla/5.0 (compatible; Academic Research Bot)")
        
        if (self$session_manager$authenticated) {
          cookies <- self$session_manager$session$driver$Network$getCookies()$result$cookies
          if (length(cookies) > 0) {
            cookie_header <- paste(
              sapply(cookies, function(c) paste0(c$name, "=", c$value)), 
              collapse = "; "
            )
            headers$Cookie <- cookie_header
            headers$Referer <- self$session_manager$session$driver$Runtime$evaluate(
              "window.location.href"
            )$result$value
          }
        }
        
        response <- GET(url, do.call(add_headers, headers), timeout(30))
        
        if (status_code(response) == 200) {
          writeBin(content(response, "raw"), dest_path)
          
          if (file.exists(dest_path) && file.size(dest_path) > 0) {
            return(list(success = TRUE, message = "Success"))
          }
        }
        
        return(list(success = FALSE, message = paste("HTTP", status_code(response))))
        
      }, error = function(e) {
        return(list(success = FALSE, message = paste("Error:", e$message)))
      })
    },
    
    batch_download = function(url_queue, progress_callback = NULL) {
      results <- list()
      download_count <- 0
      
      while (TRUE) {
        url_entry <- url_queue$get_next_url()
        if (is.null(url_entry)) break
        
        download_count <- download_count + 1
        
        # Rate limiting
        if (download_count > 1) {
          Sys.sleep(self$delay_between_downloads)
        }
        
        # Download image
        result <- self$download_image(
          url_entry$url, 
          index = download_count,
          metadata = url_entry$metadata
        )
        
        # Update queue
        url_queue$mark_processed(
          url_entry$url, 
          result$success, 
          result$message,
          result$path
        )
        
        results[[download_count]] <- c(list(url = url_entry$url), result)
        
        # Progress callback
        if (!is.null(progress_callback)) {
          progress_callback(download_count, result)
        }
        
        # Log result
        status_symbol <- if (result$success) "✓" else "✗"
        message(sprintf("%s %d: %s - %s", 
                       status_symbol, download_count, 
                       basename(url_entry$url), result$message))
      }
      
      return(results)
    }
  )
)

# ==============================================================================
# 7. PROGRESS TRACKING AND SESSION PERSISTENCE
# ==============================================================================

ProgressTracker <- R6Class("ProgressTracker",
  public = list(
    session_id = NULL,
    log_file = NULL,
    start_time = NULL,
    checkpoint_file = NULL,
    
    initialize = function(session_name = "hethport_collection") {
      self$session_id <- paste0(session_name, "_", format(Sys.time(), "%Y%m%d_%H%M%S"))
      self$log_file <- paste0(self$session_id, "_progress.log")
      self$checkpoint_file <- paste0(self$session_id, "_checkpoint.rds")
      self$start_time <- Sys.time()
      
      self$log_message("=== SESSION STARTED ===", "INFO")
      self$log_message(paste("Session ID:", self$session_id), "INFO")
    },
    
    log_message = function(message, level = "INFO") {
      timestamp <- format(Sys.time(), "%Y-%m-%d %H:%M:%S")
      log_entry <- sprintf("[%s] %s: %s", timestamp, level, message)
      
      cat(log_entry, "\n")
      cat(log_entry, "\n", file = self$log_file, append = TRUE)
    },
    
    save_checkpoint = function(current_position, total_items, queue_status = NULL, additional_data = NULL) {
      elapsed_time <- as.numeric(difftime(Sys.time(), self$start_time, units = "mins"))
      
      checkpoint_data <- list(
        session_id = self$session_id,
        timestamp = Sys.time(),
        current_position = current_position,
        total_items = total_items,
        progress_percent = round((current_position / total_items) * 100, 2),
        elapsed_time_mins = elapsed_time,
        queue_status = queue_status,
        additional_data = additional_data
      )
      
      saveRDS(checkpoint_data, self$checkpoint_file)
      
      self$log_message(sprintf(
        "CHECKPOINT: %d/%d (%.1f%%) | Elapsed: %.1f mins | ETA: %.1f mins", 
        current_position, total_items, checkpoint_data$progress_percent,
        elapsed_time,
        if (current_position > 0) elapsed_time * (total_items - current_position) / current_position else 0
      ), "CHECKPOINT")
    },
    
    load_checkpoint = function(checkpoint_file = NULL) {
      target_file <- checkpoint_file %||% self$checkpoint_file
      
      if (file.exists(target_file)) {
        checkpoint_data <- readRDS(target_file)
        self$log_message(sprintf(
          "CHECKPOINT LOADED: Session %s, Position %d (%.1f%%)", 
          checkpoint_data$session_id, checkpoint_data$current_position,
          checkpoint_data$progress_percent
        ), "RESTORE")
        return(checkpoint_data)
      }
      
      self$log_message("No checkpoint file found", "WARN")
      return(NULL)
    },
    
    generate_final_report = function(results, manifest_file = NULL) {
      # Calculate statistics
      total_attempts <- length(results)
      successful <- sum(sapply(results, function(r) r$success))
      failed <- total_attempts - successful
      success_rate <- round((successful / total_attempts) * 100, 1)
      
      elapsed_total <- as.numeric(difftime(Sys.time(), self$start_time, units = "mins"))
      
      # Generate report
      report <- sprintf("
=== COLLECTION SESSION FINAL REPORT ===
Session ID: %s
Duration: %.1f minutes
Total Attempts: %d
Successful Downloads: %d (%.1f%%)
Failed Downloads: %d
Average Rate: %.1f items/minute

Files saved in: %s
Manifest: %s
Log file: %s
Checkpoint: %s
      ", 
      self$session_id, elapsed_total, total_attempts, successful, success_rate, 
      failed, total_attempts / elapsed_total,
      if (!is.null(results[[1]]$path)) dirname(results[[1]]$path) else "N/A",
      manifest_file %||% "Not generated",
      self$log_file, self$checkpoint_file)
      
      self$log_message(report, "REPORT")
      
      # Save report to file
      report_file <- paste0(self$session_id, "_final_report.txt")
      cat(report, file = report_file)
      
      return(list(
        report_text = report,
        report_file = report_file,
        statistics = list(
          total = total_attempts,
          successful = successful,
          failed = failed,
          success_rate = success_rate,
          duration_mins = elapsed_total
        )
      ))
    }
  )
)

# ==============================================================================
# 8. MAIN WORKFLOW ORCHESTRATOR
# ==============================================================================

HethportCollector <- R6Class("HethportCollector",
  public = list(
    base_url = "https://www.hethport.uni-wuerzburg.de/",
    compliance_checker = NULL,
    session_manager = NULL,
    url_queue = NULL,
    interactive_collector = NULL,
    file_organizer = NULL,
    image_downloader = NULL,
    progress_tracker = NULL,
    
    initialize = function(fund_number = "NSF001", project_code = "HETH", output_dir = "hethport_collection") {
      message("Initializing Hethport Academic Image Collector...")
      
      # Initialize components
      self$compliance_checker <- EthicalComplianceChecker$new(self$base_url)
      self$session_manager <- SessionManager$new(headless = FALSE)  # Visual for manual interaction
      self$url_queue <- URLQueue$new(file.path(output_dir, "queue"))
      self$file_organizer <- FileOrganizer$new(output_dir, fund_number, project_code)
      self$progress_tracker <- ProgressTracker$new("hethport_collection")
      
      # Initialize collectors and downloaders
      self$interactive_collector <- InteractiveCollector$new(
        self$session_manager, self$url_queue
      )
      
      self$image_downloader <- ImageDownloader$new(
        self$session_manager, self$file_organizer, 
        max_retries = 3, delay = self$compliance_checker$get_delay()
      )
      
      message("✓ All components initialized successfully")
    },
    
    run_collection_workflow = function(start_url = NULL, require_auth = FALSE, 
                                     username = NULL, password = NULL) {
      start_url <- start_url %||% paste0(self$base_url, "HLC/")
      
      self$progress_tracker$log_message("Starting collection workflow", "INFO")
      
      # Phase 1: Authentication (if required)
      if (require_auth) {
        self$progress_tracker$log_message("Phase 1: Authentication", "PHASE")
        auth_success <- self$session_manager$authenticate(
          paste0(self$base_url, "login"), username, password
        )
        if (!auth_success) {
          stop("Authentication failed")
        }
      }
      
      # Phase 2: Interactive URL Collection
      self$progress_tracker$log_message("Phase 2: Interactive URL Collection", "PHASE")
      message("\n" %+% "="*60)
      message("PHASE 2: INTERACTIVE URL COLLECTION")
      message("Navigate through the database manually and collect image URLs.")
      message("The browser window will open for your interaction.")
      message("="*60 %+% "\n")
      
      collection_status <- self$interactive_collector$collect_urls_interactively(start_url)
      
      # Phase 3: Systematic Download
      if (collection_status$pending > 0) {
        self$progress_tracker$log_message("Phase 3: Systematic Download", "PHASE")
        message("\n" %+% "="*60)
        message(sprintf("PHASE 3: SYSTEMATIC DOWNLOAD (%d items)", collection_status$pending))
        message("="*60 %+% "\n")
        
        # Setup progress reporting
        download_results <- with_progress({
          p <- progressor(steps = collection_status$pending)
          
          self$image_downloader$batch_download(
            self$url_queue,
            progress_callback = function(count, result) {
              p(sprintf("Downloaded: %s", if(result$success) result$filename else "FAILED"))
              
              # Save checkpoint every 10 items
              if (count %% 10 == 0) {
                current_status <- self$url_queue$get_queue_status()
                self$progress_tracker$save_checkpoint(count, collection_status$pending, current_status)
              }
            }
          )
        })
        
        # Phase 4: Finalization
        self$progress_tracker$log_message("Phase 4: Finalization", "PHASE")
        manifest_file <- self$file_organizer$create_manifest()
        
        # Export final URL list
        self$url_queue$export_urls(file.path(self$file_organizer$base_dir, "documentation", "all_urls.csv"))
        
        # Generate final report
        final_report <- self$progress_tracker$generate_final_report(download_results, manifest_file)
        
        message("\n" %+% "="*60)
        message("COLLECTION WORKFLOW COMPLETE")
        message("="*60)
        cat(final_report$report_text)
        
        return(list(
          status = "completed",
          results = download_results,
          manifest = manifest_file,
          report = final_report,
          statistics = final_report$statistics
        ))
        
      } else {
        self$progress_tracker$log_message("No URLs collected, workflow terminated", "WARN")
        return(list(status = "no_urls_collected"))
      }
    },
    
    resume_from_checkpoint = function(checkpoint_file) {
      checkpoint <- self$progress_tracker$load_checkpoint(checkpoint_file)
      if (is.null(checkpoint)) {
        stop("Could not load checkpoint file")
      }
      
      # Resume download process
      remaining_status <- self$url_queue$get_queue_status()
      if (remaining_status$pending > 0) {
        message(sprintf("Resuming collection: %d items remaining", remaining_status$pending))
        return(self$run_collection_workflow())
      } else {
        message("No pending items to process")
        return(list(status = "no_pending_items"))
      }
    }
  )
)

# ==============================================================================
# 9. CITATION AND DOCUMENTATION GENERATOR
# ==============================================================================

CitationGenerator <- R6Class("CitationGenerator",
  public = list(
    project_info = NULL,
    
    initialize = function(project_title, principal_investigator, institution, 
                         fund_number, fund_agency = NULL) {
      self$project_info <- list(
        title = project_title,
        pi = principal_investigator,
        institution = institution,
        fund_number = fund_number,
        fund_agency = fund_agency,
        collection_date = Sys.Date()
      )
    },
    
    generate_dataset_citation = function(total_images, source_database = "Hethport Cuneiform Database") {
      citation <- sprintf(
        "%s (%s). %s [Dataset of %d cuneiform images]. Collected from %s. %s. Fund: %s %s.",
        self$project_info$pi,
        format(self$project_info$collection_date, "%Y"),
        self$project_info$title,
        total_images,
        source_database,
        self$project_info$institution,
        if (!is.null(self$project_info$fund_agency)) self$project_info$fund_agency else "",
        self$project_info$fund_number
      )
      
      return(citation)
    },
    
    generate_methodology_documentation = function(output_file = "methodology.md") {
      doc <- sprintf("
# Data Collection Methodology

## Project Information
- **Title**: %s
- **Principal Investigator**: %s
- **Institution**: %s
- **Funding**: %s %s
- **Collection Date**: %s

## Methodology
This dataset was collected using a semi-automated approach that combines manual navigation with programmatic assistance, designed to respect robots.txt restrictions while enabling systematic academic research.

### Compliance Framework
- **Robots.txt Compliance**: All collection activities respected robots.txt directives
- **Rate Limiting**: Implemented %d-second delays between requests
- **Authentication**: %s
- **Legal Basis**: Academic fair use for research purposes

### Technical Implementation
- **Browser Automation**: Selenider/Chromote for R
- **Session Management**: Persistent cookie storage and authentication state
- **Quality Control**: Multi-level validation and error handling
- **File Organization**: Systematic naming with fund number identification
- **Progress Tracking**: Comprehensive logging and checkpoint system

### Data Quality Assurance
- Manual verification of collected URLs
- Automated duplicate detection and removal
- Comprehensive metadata capture and preservation
- Error logging and retry mechanisms with exponential backoff

## Reproducibility
This collection methodology is fully documented and reproducible. All code and protocols are available in the associated research compendium.

### File Naming Convention
Files are named using the pattern: `PROJECT_FUND_DATE_SEQUENCE_TYPE_VERSION.ext`
- PROJECT: %s
- FUND: %s
- DATE: YYYYMMDD format
- SEQUENCE: 6-digit sequential identifier with leading zeros
- TYPE: Content type (IMG, META, etc.)
- VERSION: Version identifier (v01, v02, etc.)

## Ethical Considerations
This data collection was conducted in accordance with:
- Academic research ethics guidelines
- Institutional review board requirements (where applicable)  
- Professional archaeological ethics codes
- International copyright and fair use provisions
- Database terms of service and usage policies

## Citation
When using this dataset, please cite as:
%s

## Contact
For questions about this methodology or dataset, contact:
%s, %s
      ",
      self$project_info$title,
      self$project_info$pi,
      self$project_info$institution,
      if (!is.null(self$project_info$fund_agency)) self$project_info$fund_agency else "",
      self$project_info$fund_number,
      self$project_info$collection_date,
      2,  # Default delay
      "Manual authentication with session persistence",
      strsplit(self$project_info$fund_number, "_")[[1]][1],  # Project code
      self$project_info$fund_number,
      self$generate_dataset_citation(0, "Hethport Cuneiform Database"),
      self$project_info$pi,
      self$project_info$institution
      )
      
      writeLines(doc, output_file)
      message(sprintf("Methodology documentation saved to: %s", output_file))
      return(output_file)
    },
    
    generate_readme = function(output_file = "README.md") {
      readme <- sprintf("
# %s

Systematic collection of cuneiform images from the Hethport database for academic research.

## Quick Start

```r
# Initialize collector
collector <- HethportCollector$new(
  fund_number = \"%s\",
  project_code = \"%s\",
  output_dir = \"hethport_collection\"
)

# Run collection workflow
results <- collector$run_collection_workflow(
  start_url = \"https://www.hethport.uni-wuerzburg.de/HLC/\",
  require_auth = FALSE
)
```

## Directory Structure

```
hethport_collection/
├── images/              # Downloaded image files
├── metadata/            # Individual file metadata (JSON)
├── documentation/       # Project documentation and manifests  
├── logs/               # Session logs and progress tracking
└── queue/              # URL queue management files
```

## Principal Investigator
%s  
%s  

## Funding
%s %s

## Collection Date
%s

## License and Usage
This dataset is intended for academic research purposes. Please respect the original database terms of service and provide appropriate attribution when using these materials in research or publication.

## Technical Requirements
- R (>= 4.0.0)  
- Required packages: selenider, chromote, polite, robotstxt, httr, jsonlite, readr, dplyr, stringr, digest, progressr, R6, lubridate, magrittr

## Support
For technical support or questions about this collection, please contact the research team.
      ",
      self$project_info$title,
      self$project_info$fund_number,
      strsplit(self$project_info$fund_number, "_")[[1]][1],
      self$project_info$pi,
      self$project_info$institution,
      if (!is.null(self$project_info$fund_agency)) self$project_info$fund_agency else "",
      self$project_info$fund_number,
      self$project_info$collection_date
      )
      
      writeLines(readme, output_file)
      message(sprintf("README documentation saved to: %s", output_file))
      return(output_file)
    }
  )
)

# ==============================================================================
# 10. USAGE EXAMPLES AND WORKFLOW DEMONSTRATION
# ==============================================================================

# Example 1: Basic Collection Workflow
demo_basic_collection <- function() {
  message("=== HETHPORT COLLECTION DEMO ===")
  
  # Initialize collector with your project details
  collector <- HethportCollector$new(
    fund_number = "NSF2024001",  # Your grant/fund number
    project_code = "HETH",       # Your project abbreviation
    output_dir = "demo_collection"
  )
  
  # Run the complete workflow
  results <- collector$run_collection_workflow(
    start_url = "https://www.hethport.uni-wuerzburg.de/HLC/",
    require_auth = FALSE  # Set to TRUE if authentication required
  )
  
  return(results)
}

# Example 2: Resuming from Checkpoint
demo_resume_collection <- function(checkpoint_file) {
  collector <- HethportCollector$new(
    fund_number = "NSF2024001",
    project_code = "HETH",
    output_dir = "demo_collection"
  )
  
  results <- collector$resume_from_checkpoint(checkpoint_file)
  return(results)
}

# Example 3: Generate Project Documentation
demo_documentation_generation <- function() {
  # Initialize citation generator
  citation_gen <- CitationGenerator$new(
    project_title = "Systematic Analysis of Hittite Cuneiform Inscriptions",
    principal_investigator = "Dr. Jane Scholar",
    institution = "University Research Institute",
    fund_number = "NSF2024001",
    fund_agency = "National Science Foundation"
  )
  
  # Generate documentation
  citation_gen$generate_methodology_documentation("methodology_demo.md")
  citation_gen$generate_readme("README_demo.md")
  
  # Generate dataset citation
  citation <- citation_gen$generate_dataset_citation(
    total_images = 150,
    source_database = "Hethport Cuneiform Database"
  )
  
  message("Generated citation:")
  message(citation)
  
  return(citation)
}

# ==============================================================================
# FINAL EXECUTION INSTRUCTIONS
# ==============================================================================

message("
=============================================================================
HETHPORT ACADEMIC IMAGE COLLECTION FRAMEWORK - READY FOR USE
=============================================================================

This comprehensive framework provides ethical, systematic collection of
cuneiform images from the Hethport database while respecting robots.txt 
restrictions and maintaining academic research standards.

QUICK START:
1. Run: demo_basic_collection() for a complete demonstration
2. Customize fund_number and project_code for your research
3. Follow interactive prompts for URL collection
4. Review generated documentation and citations

KEY FEATURES:
✓ Robots.txt compliance checking and rate limiting
✓ Semi-automated browser session management
✓ Interactive URL collection with manual verification
✓ Systematic file naming with fund number integration
✓ Comprehensive progress tracking and session persistence
✓ Academic documentation and citation generation
✓ Robust error handling and recovery mechanisms

ETHICAL COMPLIANCE:
✓ Respects robots.txt directives and crawl delays
✓ Manual oversight prevents unauthorized access
✓ Academic fair use framework implementation
✓ Comprehensive logging for accountability
✓ Professional research ethics compliance

For support or questions, consult the generated methodology documentation.
=============================================================================
")